{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Techniques"
      ],
      "metadata": {
        "id": "e02NuI9YGyNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions And Answer"
      ],
      "metadata": {
        "id": "Cu3dMxy1G1nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Ensemble Learning in machine learning? Explain the key idea behind it.\n",
        "- Ensemble Learning is a machine learning technique where multiple models are trained and combined to make a final prediction. Instead of relying on a single model, ensemble methods aggregate the predictions of several models to improve accuracy, stability, and robustness.\n",
        "\n",
        "The key idea behind ensemble learning is that a group of diverse and relatively weak models, when combined, can produce better results than any single model alone. By reducing bias, variance, or both, ensemble learning leads to improved generalization on unseen data."
      ],
      "metadata": {
        "id": "kBbzfnafG4on"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Question 2: What is the difference between Bagging and Boosting?\n",
        " - Bagging (Bootstrap Aggregating) trains multiple models independently using different random samples of the dataset and then combines their predictions, usually by averaging or voting. Its main goal is to reduce variance and prevent overfitting.\n",
        "\n",
        "- Boosting, on the other hand, trains models sequentially, where each new model focuses more on correcting the errors made by previous models. Boosting aims to reduce both bias and variance and gives more importance to difficult or misclassified data points."
      ],
      "metadata": {
        "id": "Fbl3-7i9HF_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "- Bootstrap sampling is a technique where multiple training datasets are created by randomly sampling the original dataset with replacement. This means some data points may appear multiple times in a sample, while others may not appear at all.\n",
        "\n",
        "- In Bagging methods like Random Forest, bootstrap sampling ensures diversity among individual trees. Each tree is trained on a different bootstrap sample, which helps reduce variance and improves the overall performance of the ensemble."
      ],
      "metadata": {
        "id": "STNH7bVAHPWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "- Out-of-Bag (OOB) samples are the data points that are not selected in a particular bootstrap sample. On average, about 37% of the data remains unused for each tree.\n",
        "\n",
        "- The OOB score is calculated by predicting these unused samples using the trees that did not see them during training. It provides an unbiased estimate of model performance without needing a separate validation dataset."
      ],
      "metadata": {
        "id": "vR1R-IZLHdg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "- In a single Decision Tree, feature importance is based on how much each feature reduces impurity at each split. However, this can be unstable because small changes in data may lead to very different trees.\n",
        "\n",
        "- In a Random Forest, feature importance is averaged across many trees, making it more reliable and stable. Random Forest reduces bias toward specific features and provides a more robust estimate of feature importance."
      ],
      "metadata": {
        "id": "DpmFzK8qHqYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to: ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier ● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': rf.feature_importances_\n",
        "})\n",
        "\n",
        "# Top 5 features\n",
        "top_5 = feature_importance.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(top_5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5H4sJaTH-OK",
        "outputId": "d89abccd-0632-4e37-b07e-01ca993c6ee1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to: ● Train a Bagging Classifier using Decision Trees on the Iris dataset ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(), # Changed base_estimator to estimator\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PZ0BPV_IXad",
        "outputId": "89bcc91c-bf39-458b-b493-b66da982c496"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to: ● Train a Random Forest Classifier ● Tune hyperparameters max_depth and n_estimators using GridSearchCV ● Print the best parameters and final accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Grid Search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100],\n",
        "    'max_depth': [None, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(rf, param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy_score(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB9wjr5BI-mL",
        "outputId": "3668d1c1-3c81-4414-c5f7-fc6ab6aeb7bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to: ● Load the California Housing dataset\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bag = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(), # Changed base_estimator to estimator\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_pred = rf.predict(X_test)\n",
        "\n",
        "print(\"Bagging MSE:\", mean_squared_error(y_test, bag_pred))\n",
        "print(\"Random Forest MSE:\", mean_squared_error(y_test, rf_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VO8T7gRJK7B",
        "outputId": "46d02a90-34e0-416a-a669-cfb373897502"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.25787382250585034\n",
            "Random Forest MSE: 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Ensemble Learning for Loan Default Prediction\n",
        "\n",
        "- Choosing Bagging or Boosting:\n",
        "Boosting would be preferred because loan default prediction involves complex patterns and misclassification costs are high. Boosting focuses on difficult cases.\n",
        "\n",
        "- Handling Overfitting:\n",
        "Overfitting can be handled using cross-validation, limiting tree depth, regularization, and early stopping in boosting algorithms.\n",
        "\n",
        "- Selecting Base Models:\n",
        "Decision Trees are chosen as base learners because they handle non-linear relationships and mixed data types effectively.\n",
        "\n",
        "- Evaluating Performance:\n",
        "K-fold cross-validation is used along with metrics such as ROC-AUC, precision, recall, and F1-score to ensure robust evaluation.\n",
        "\n",
        "- Justification of Ensemble Learning:\n",
        "Ensemble learning improves decision-making by combining multiple perspectives, reducing individual model errors, and increasing predictive reliability. This leads to better risk assessment and more informed loan approval decisions."
      ],
      "metadata": {
        "id": "pDIuTnSVJhgW"
      }
    }
  ]
}